{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd1fb96",
   "metadata": {},
   "source": [
    "# Aegis-C9 ML Prediction Model\n",
    "\n",
    "This notebook builds a machine learning model to predict match outcomes for the Aegis-C9 esports coaching platform.\n",
    "\n",
    "## Features:\n",
    "- Win probability prediction based on player statistics\n",
    "- Player performance analysis\n",
    "- Real-time prediction API-ready model export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ec66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b0df5",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the League of Legends match data\n",
    "df = pd.read_csv('League of Legends Ranked Match Data  Season 15 (EUN).csv')\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nColumns ({len(df.columns)} total):\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows and basic info\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0902bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a983036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_data = df.isnull().sum()\n",
    "missing_data[missing_data > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3141f18",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a201e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features for win prediction\n",
    "feature_columns = [\n",
    "    'kills', 'deaths', 'assists', 'kda_ratio', 'kill_participation',\n",
    "    'gold_earned', 'gold_per_min', 'damage_dealt', 'damage_per_min',\n",
    "    'damage_to_champ', 'damage_champ_per_min', 'damage_taken',\n",
    "    'vision_score', 'team_baronKills', 'team_dragonKills',\n",
    "    'team_towerKills', 'team_champKills', 'team_riftHeraldKills',\n",
    "    'team_inhibitorKills', 'duration'\n",
    "]\n",
    "\n",
    "# Create a working copy with relevant columns\n",
    "df_model = df[feature_columns + ['win', 'position', 'game_id']].copy()\n",
    "\n",
    "# Convert 'win' to numeric (TRUE -> 1, FALSE -> 0)\n",
    "df_model['win'] = df_model['win'].map({True: 1, False: 0, 'TRUE': 1, 'FALSE': 0})\n",
    "\n",
    "print(f\"Working dataset shape: {df_model.shape}\")\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c671e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate player data by game (team-level statistics)\n",
    "agg_functions = {\n",
    "    'kills': 'sum',\n",
    "    'deaths': 'sum',\n",
    "    'assists': 'sum',\n",
    "    'kda_ratio': 'mean',\n",
    "    'kill_participation': 'mean',\n",
    "    'gold_earned': 'sum',\n",
    "    'gold_per_min': 'mean',\n",
    "    'damage_dealt': 'sum',\n",
    "    'damage_per_min': 'mean',\n",
    "    'damage_to_champ': 'sum',\n",
    "    'damage_champ_per_min': 'mean',\n",
    "    'damage_taken': 'sum',\n",
    "    'vision_score': 'sum',\n",
    "    'team_baronKills': 'first',\n",
    "    'team_dragonKills': 'first',\n",
    "    'team_towerKills': 'first',\n",
    "    'team_champKills': 'first',\n",
    "    'team_riftHeraldKills': 'first',\n",
    "    'team_inhibitorKills': 'first',\n",
    "    'duration': 'first',\n",
    "    'win': 'first'\n",
    "}\n",
    "\n",
    "# Group by game_id and win status to get team-level data\n",
    "df_team = df_model.groupby(['game_id', 'win']).agg(agg_functions).reset_index()\n",
    "\n",
    "print(f\"Team-level dataset shape: {df_team.shape}\")\n",
    "df_team.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle any missing values\n",
    "df_team = df_team.fillna(df_team.median())\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Win/Loss Distribution:\")\n",
    "print(df_team['win'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f211054b",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ef32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key features by win/loss\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Key Metrics by Win/Loss Outcome', fontsize=14, fontweight='bold')\n",
    "\n",
    "metrics = ['kills', 'gold_earned', 'damage_to_champ', 'vision_score', 'team_dragonKills', 'team_baronKills']\n",
    "colors = ['#00aeef', '#ff6b35']\n",
    "\n",
    "for ax, metric in zip(axes.flatten(), metrics):\n",
    "    df_team.boxplot(column=metric, by='win', ax=ax)\n",
    "    ax.set_title(metric.replace('_', ' ').title())\n",
    "    ax.set_xlabel('Win (0=Loss, 1=Win)')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a7169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Select numeric columns for correlation\n",
    "numeric_cols = df_team.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = df_team[numeric_cols].corr()\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation with win\n",
    "win_correlation = corr_matrix['win'].drop('win').sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#00aeef' if x > 0 else '#ff6b35' for x in win_correlation.values]\n",
    "win_correlation.plot(kind='barh', color=colors)\n",
    "plt.title('Feature Correlation with Win Outcome', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715944c2",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "feature_cols = [col for col in df_team.columns if col not in ['game_id', 'win']]\n",
    "X = df_team[feature_cols]\n",
    "y = df_team['win']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c2b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Use scaled data for logistic regression, original for tree-based\n",
    "    if name == 'Logistic Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'roc_auc': roc_auc,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_prob\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050223b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison visualization\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[m]['accuracy'] for m in model_names]\n",
    "roc_aucs = [results[m]['roc_auc'] for m in model_names]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', color='#00aeef')\n",
    "bars2 = ax.bar(x + width/2, roc_aucs, width, label='ROC-AUC', color='#ff6b35')\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_names, rotation=15)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1 + bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abdef87",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest (best performer typically)\n",
    "rf_model = results['Random Forest']['model']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['#00aeef' if i < 10 else '#94a3b8' for i in range(len(feature_importance))]\n",
    "plt.barh(feature_importance['feature'], feature_importance['importance'], color=colors)\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Feature Importance (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "feature_importance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8541ea6a",
   "metadata": {},
   "source": [
    "## 6. Real-Time Win Probability Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model\n",
    "best_model_name = max(results.keys(), key=lambda k: results[k]['roc_auc'])\n",
    "best_model = results[best_model_name]['model']\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"ROC-AUC: {results[best_model_name]['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fedd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_win_probability(game_stats: dict, model=best_model, feature_columns=feature_cols):\n",
    "    \"\"\"\n",
    "    Predict the win probability based on current game statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    game_stats : dict\n",
    "        Dictionary containing current game statistics\n",
    "        Required keys: kills, deaths, assists, gold_earned, etc.\n",
    "    model : trained sklearn model\n",
    "        The prediction model to use\n",
    "    feature_columns : list\n",
    "        List of feature column names in correct order\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Contains 'win_probability' and 'prediction'\n",
    "    \"\"\"\n",
    "    # Create DataFrame with correct column order\n",
    "    input_data = pd.DataFrame([game_stats])[feature_columns]\n",
    "    \n",
    "    # Make prediction\n",
    "    win_prob = model.predict_proba(input_data)[0][1]\n",
    "    prediction = 'Win' if win_prob >= 0.5 else 'Loss'\n",
    "    \n",
    "    return {\n",
    "        'win_probability': round(win_prob * 100, 2),\n",
    "        'prediction': prediction,\n",
    "        'confidence': round(abs(win_prob - 0.5) * 200, 2)\n",
    "    }\n",
    "\n",
    "# Example usage with mock data\n",
    "example_game = {\n",
    "    'kills': 15,\n",
    "    'deaths': 8,\n",
    "    'assists': 25,\n",
    "    'kda_ratio': 3.5,\n",
    "    'kill_participation': 0.65,\n",
    "    'gold_earned': 55000,\n",
    "    'gold_per_min': 450,\n",
    "    'damage_dealt': 150000,\n",
    "    'damage_per_min': 1200,\n",
    "    'damage_to_champ': 80000,\n",
    "    'damage_champ_per_min': 650,\n",
    "    'damage_taken': 70000,\n",
    "    'vision_score': 120,\n",
    "    'team_baronKills': 1,\n",
    "    'team_dragonKills': 3,\n",
    "    'team_towerKills': 6,\n",
    "    'team_champKills': 20,\n",
    "    'team_riftHeraldKills': 1,\n",
    "    'team_inhibitorKills': 2,\n",
    "    'duration': 1800\n",
    "}\n",
    "\n",
    "result = predict_win_probability(example_game)\n",
    "print(\"\\n=== Prediction Result ===\")\n",
    "print(f\"Win Probability: {result['win_probability']}%\")\n",
    "print(f\"Predicted Outcome: {result['prediction']}\")\n",
    "print(f\"Confidence: {result['confidence']}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd68d20",
   "metadata": {},
   "source": [
    "## 7. Player Performance Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f47268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create player-level performance metrics\n",
    "df_player = df[[\n",
    "    'participant_id', 'champion_name', 'position', 'win',\n",
    "    'kills', 'deaths', 'assists', 'kda_ratio', 'kill_participation',\n",
    "    'gold_per_min', 'damage_champ_per_min', 'vision_score'\n",
    "]].copy()\n",
    "\n",
    "# Convert win to numeric\n",
    "df_player['win'] = df_player['win'].map({True: 1, False: 0, 'TRUE': 1, 'FALSE': 0})\n",
    "\n",
    "# Calculate performance score\n",
    "df_player['performance_score'] = (\n",
    "    df_player['kda_ratio'] * 0.3 +\n",
    "    df_player['kill_participation'] * 100 * 0.2 +\n",
    "    df_player['gold_per_min'] / 50 * 0.2 +\n",
    "    df_player['damage_champ_per_min'] / 100 * 0.2 +\n",
    "    df_player['vision_score'] / 5 * 0.1\n",
    ")\n",
    "\n",
    "df_player.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae96f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by position\n",
    "position_stats = df_player.groupby('position').agg({\n",
    "    'performance_score': 'mean',\n",
    "    'kda_ratio': 'mean',\n",
    "    'win': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "position_stats['performance_score'].plot(kind='bar', color='#00aeef')\n",
    "plt.title('Average Performance Score by Position', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Performance Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "position_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f8185e",
   "metadata": {},
   "source": [
    "## 8. Export Model for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'aegis_win_prediction_model.pkl')\n",
    "joblib.dump(scaler, 'aegis_feature_scaler.pkl')\n",
    "\n",
    "# Save feature columns configuration\n",
    "config = {\n",
    "    'feature_columns': feature_cols,\n",
    "    'model_name': best_model_name,\n",
    "    'model_accuracy': results[best_model_name]['accuracy'],\n",
    "    'model_roc_auc': results[best_model_name]['roc_auc']\n",
    "}\n",
    "\n",
    "with open('aegis_model_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Model exported successfully!\")\n",
    "print(f\"- Model file: aegis_win_prediction_model.pkl\")\n",
    "print(f\"- Scaler file: aegis_feature_scaler.pkl\")\n",
    "print(f\"- Config file: aegis_model_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6846a9d",
   "metadata": {},
   "source": [
    "## 9. API-Ready Prediction Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16af254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AegisPredictionEngine:\n",
    "    \"\"\"\n",
    "    Production-ready prediction engine for Aegis-C9 platform.\n",
    "    Can be deployed as an API service.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path='aegis_win_prediction_model.pkl', \n",
    "                 config_path='aegis_model_config.json'):\n",
    "        self.model = joblib.load(model_path)\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.config = json.load(f)\n",
    "        self.feature_columns = self.config['feature_columns']\n",
    "    \n",
    "    def predict(self, game_stats: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Make a win probability prediction.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        game_stats : dict\n",
    "            Current game statistics\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict with keys:\n",
    "            - win_probability: float (0-100)\n",
    "            - prediction: str ('Win' or 'Loss')\n",
    "            - confidence: float (0-100)\n",
    "            - risk_level: str ('Low', 'Medium', 'High')\n",
    "        \"\"\"\n",
    "        input_data = pd.DataFrame([game_stats])[self.feature_columns]\n",
    "        win_prob = self.model.predict_proba(input_data)[0][1]\n",
    "        \n",
    "        # Determine risk level\n",
    "        if win_prob >= 0.7:\n",
    "            risk_level = 'Low'\n",
    "        elif win_prob >= 0.4:\n",
    "            risk_level = 'Medium'\n",
    "        else:\n",
    "            risk_level = 'High'\n",
    "        \n",
    "        return {\n",
    "            'win_probability': round(win_prob * 100, 2),\n",
    "            'prediction': 'Win' if win_prob >= 0.5 else 'Loss',\n",
    "            'confidence': round(abs(win_prob - 0.5) * 200, 2),\n",
    "            'risk_level': risk_level,\n",
    "            'model_info': {\n",
    "                'name': self.config['model_name'],\n",
    "                'accuracy': self.config['model_accuracy']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_feature_requirements(self) -> list:\n",
    "        \"\"\"Return the list of required features for prediction.\"\"\"\n",
    "        return self.feature_columns\n",
    "\n",
    "# Test the prediction engine\n",
    "engine = AegisPredictionEngine()\n",
    "prediction = engine.predict(example_game)\n",
    "\n",
    "print(\"\\n=== Aegis Prediction Engine Test ===\")\n",
    "print(json.dumps(prediction, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d23145",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has created:\n",
    "\n",
    "1. **Win Probability Model** - Predicts match outcomes based on in-game statistics\n",
    "2. **Feature Importance Analysis** - Identifies key factors affecting win probability\n",
    "3. **Player Performance Scoring** - Evaluates individual player contributions\n",
    "4. **Production-Ready Export** - Model files ready for API deployment\n",
    "\n",
    "### Integration with Aegis-C9 Frontend:\n",
    "- The exported model can be loaded in a Python backend/API\n",
    "- Real-time predictions can be sent to the frontend via WebSocket\n",
    "- The win probability displayed on the dashboard can use these predictions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
